{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python380jvsc74a57bd0ffcfff3091a847c16fc07470374681ed3f05b5fd16c9abde2e7fb436a6da7cf3",
   "display_name": "Python 3.8.0 64-bit ('attitude': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from env.Helicopter import Helicopter\n",
    "from env.controller import Controller\n",
    "from finding_random_states_and_actions import state_finder\n",
    "from simple_pid import PID\n",
    "import numpy as np\n",
    "\n",
    "my_state_finder = state_finder()\n",
    "my_heli = Helicopter()\n",
    "my_contr = Controller()\n",
    "ENV_ID = \"CustomEnv-v0\"\n",
    "my_env = gym.make(ENV_ID)\n",
    "current_best_rew = -100000000000\n",
    "\n",
    "# print(sl_action)\n",
    "done = False\n",
    "# [0.8,0,0] lateral\n",
    "\n",
    "k_bin = np.linspace(-1, 1, num=10)\n",
    "i_bin = np.linspace(-1, 1, num=5)\n",
    "\n",
    "current_action = [0, 0, 0, 0]\n",
    "sl_action = [1, 10, 10, 5, 5, 1, 1, 2, 2, 1, 1, 1, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "state_diverge\nbest reward: -inf, longest step: 17, reward: [-236.93157518], step: 14 \n"
     ]
    }
   ],
   "source": [
    "# pid_lon = PID(kk, ii, 0, setpoint=-1.08e-01)\n",
    "my_env.best_reward = float('-inf')\n",
    "sl_ctrl = [0,0,0,0]\n",
    "pid_lon = PID(0.2, 0.2, 0.01, setpoint=1.01e-04)  # controls q, theta\n",
    "pid_lat = PID(0.2, 0.1, 0.1, setpoint=-1.08e-01)  # controls p, fi\n",
    "pid_col = PID(-10, 0, 0, setpoint=0)\n",
    "# pid_col = PID(-1, -0.1, -0.234, setpoint=0)\n",
    "pid_ped = PID(-2.5, -1.10, -0.7, setpoint=-1.03e-03)\n",
    "my_env.current_states = my_env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    # Yd, Ydotd, Ydotdotd, Y, Ydot = my_contr.Yposition(my_env.dt * my_env.counter, my_env.current_states)\n",
    "\n",
    "    # sl_ctrl = my_contr.Controller_model(my_env.current_states, my_env.dt * my_env.counter, action=sl_action)\n",
    "    sl_ctrl[0] = float(pid_col(my_env.current_states[11]))\n",
    "    sl_ctrl[1] = float(pid_lat(my_env.current_states[6]))\n",
    "    sl_ctrl[2] = float(pid_lon(my_env.current_states[7]))\n",
    "    sl_ctrl[3] = float(pid_ped(my_env.current_states[8]))\n",
    "    states, b, done, _ = my_env.step(sl_ctrl)\n",
    "    constant_dict = {\n",
    "        \"u\": 1., \"v\": 1., \"w\": 0.,\n",
    "        \"p\": 0., \"q\": 0., \"r\": 0.,\n",
    "        \"fi\": 0., \"theta\": 0., \"si\": 0.,\n",
    "        \"x\": 1., \"y\": 1., \"z\": 0.,\n",
    "        \"a\": 0., \"b\": 0., \"c\": 0., \"d\": 0.,}\n",
    "    my_env.make_constant(list(constant_dict.values()))\n",
    "if my_env.best_reward > current_best_rew:\n",
    "    current_best_rew = my_env.best_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_env.best_reward = float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pid_lon = PID(kk, ii, 0, setpoint=-1.08e-01)\n",
    "sl_ctrl = [0,0,0,0]\n",
    "pid_lon = PID(0.2, 0.2, 0.01, setpoint=1.01e-04)  # controls q, theta\n",
    "pid_lat = PID(0.2, 0.1, 0.01, setpoint=-1.08e-01)  # controls p, fi\n",
    "pid_col = PID(-1.2, -3.5, -0.234, setpoint=0)\n",
    "# pid_col = PID(-1, -0.1, -0.234, setpoint=0)\n",
    "pid_ped = PID(-2.5, -1.10, -0.7, setpoint=-1.03e-03)\n",
    "my_env.current_states = my_env.reset()\n",
    "done = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yd, Ydotd, Ydotdotd, Y, Ydot = my_contr.Yposition(my_env.dt * my_env.counter, my_env.current_states)\n",
    "\n",
    "# sl_ctrl = my_contr.Controller_model(my_env.current_states, my_env.dt * my_env.counter, action=sl_action)\n",
    "sl_ctrl[0] = float(pid_col(my_env.current_states[11]))\n",
    "sl_ctrl[1] = float(pid_lat(my_env.current_states[6]))\n",
    "sl_ctrl[2] = float(pid_lon(my_env.current_states[7]))\n",
    "sl_ctrl[3] = float(pid_ped(my_env.current_states[8]))\n",
    "states, b, done, _ = my_env.step(sl_ctrl)\n",
    "constant_dict = {\n",
    "    \"u\": 1., \"v\": 1., \"w\": 0.,\n",
    "    \"p\": 0., \"q\": 0., \"r\": 0.,\n",
    "    \"fi\": 0., \"theta\": 0., \"si\": 0.,\n",
    "    \"x\": 1., \"y\": 1., \"z\": 0.,\n",
    "    \"a\": 0., \"b\": 0., \"c\": 0., \"d\": 0.,}\n",
    "my_env.make_constant(list(constant_dict.values()))\n",
    "if my_env.best_reward > current_best_rew:\n",
    "    current_best_rew = my_env.best_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[-0.025398357008214845,\n",
       " -0.06381338546655237,\n",
       " 1.7463657821744332,\n",
       " -5.425710061108241,\n",
       " -0.26874454124852,\n",
       " -0.021657637246068326,\n",
       " -0.2507295158720202,\n",
       " -0.00212494280028515,\n",
       " 0.02245921475037266,\n",
       " 0.007601609867288977,\n",
       " -0.0015830049006509612,\n",
       " 0.028833010676213154,\n",
       " -0.025514892908812564,\n",
       " -0.08498086697795085,\n",
       " 0.004245792201218974,\n",
       " 0.21402739244173705]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "states"
   ]
  }
 ]
}